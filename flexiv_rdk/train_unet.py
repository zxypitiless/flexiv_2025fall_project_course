import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from ropewrap.data.rgbd_dataset.rod_dataset import RodDataset
from ropewrap.unet import UNet
import os
import re

# ============================
#   è®­ç»ƒå‚æ•°
# ============================
BATCH_SIZE = 2
LR = 1e-4
EPOCHS = 100
SAVE_DIR = "checkpoints"

os.makedirs(SAVE_DIR, exist_ok=True)


# ============================================
#   è‡ªåŠ¨æ£€æµ‹æœ€æ–°æ¨¡å‹ï¼ˆå¦‚ unet_epoch_40.ptï¼‰
# ============================================
def find_latest_checkpoint():
    files = os.listdir(SAVE_DIR)

    # åŒ¹é… unet_epoch_xx.pt
    ckpts = []
    for f in files:
        m = re.match(r"unet_epoch_(\d+)\.pt", f)
        if m:
            epoch = int(m.group(1))
            ckpts.append((epoch, f))

    if not ckpts:
        return None, 0

    # æ ¹æ® epoch æœ€å¤§å€¼æ’åº
    ckpts.sort(key=lambda x: x[0], reverse=True)

    latest_epoch, latest_file = ckpts[0]
    return os.path.join(SAVE_DIR, latest_file), latest_epoch


def train():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("ä½¿ç”¨è®¾å¤‡:", device)

    # æ•°æ®é›†
    dataset = RodDataset("data/rgbd_dataset")
    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

    # åˆå§‹åŒ–æ¨¡å‹
    model = UNet(n_channels=3, n_classes=3).to(device)

    # ============================================
    #   æ£€æŸ¥å¹¶åŠ è½½æœ€è¿‘çš„ checkpoint
    # ============================================
    ckpt_path, start_epoch = find_latest_checkpoint()

    if ckpt_path:
        print(f"ğŸ” æ£€æµ‹åˆ°æœ€è¿‘æ¨¡å‹ï¼š{ckpt_path}")
        checkpoint = torch.load(ckpt_path, map_location=device)
        model.load_state_dict(checkpoint["model"])
        print(f"ğŸ‘‰ ä» epoch {start_epoch} ç»§ç»­è®­ç»ƒ\n")
    else:
        print("æœªå‘ç° checkpointï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚\n")
        start_epoch = 0

    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=LR)

    # ============================================
    #   æ­£å¼è®­ç»ƒ
    # ============================================
    for epoch in range(start_epoch, start_epoch + EPOCHS):
        model.train()
        total_loss = 0

        for rgb, depth, masks in loader:
            # if epoch == start_epoch :
                # print("Mask min:", masks.min().item())
                # print("Mask max:", masks.max().item())
                # print("Mask unique:", torch.unique(masks))
                # print("Mask shape:", masks.shape)  # åº”æ˜¯ B,3,H,W
                # print("Mask sum:", masks.sum(dim=(1,2)))  # ä¸‰ä¸ª channel çš„åƒç´ æ€»æ•°


            rgb = rgb.float().permute(0, 3, 1, 2).to(device) / 255.0
            masks = masks.float().to(device)

            optimizer.zero_grad()
            pred = model(rgb)
            loss = criterion(pred, masks)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        print(f"[Epoch {epoch+1}] Loss = {total_loss / len(loader):.4f}")

        # ============================================
        #   ğŸ”¥ æ¯ 10 ä¸ª epoch ä¿å­˜ä¸€æ¬¡ï¼Œä¸è¦†ç›–æ—§æ¨¡å‹
        # ============================================
        if (epoch + 1) % 10 == 0:
            save_path = os.path.join(SAVE_DIR, f"unet_epoch_{epoch+1}.pt")
            torch.save({
                "epoch": epoch + 1,
                "model": model.state_dict(),
            }, save_path)
            print(f"âœ” å·²ä¿å­˜æ¨¡å‹ï¼š{save_path}")
    



    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    train()
